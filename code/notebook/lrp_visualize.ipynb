{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run relevance backout here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from model.BERT import *\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from util.optimization import BERTAdam\n",
    "from util.processor import *\n",
    "\n",
    "from util.tokenization import *\n",
    "\n",
    "from util.evaluation import *\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# this imports most of the helpers needed to eval the model\n",
    "from run_classifier import *\n",
    "\n",
    "lrp_data_dir = \"../../results\"\n",
    "vocab_data_dir = \"../../data/uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "sys.path.append(\"..\")\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/30/2020 01:23:11 - INFO - run_classifier -   gpu is out of the picture, let us use CPU\n"
     ]
    }
   ],
   "source": [
    "# Note that this notebook only supports single GPU evaluation\n",
    "# which is sufficient for most of tasks by using lower batch size.\n",
    "IS_CUDA = False\n",
    "if IS_CUDA:\n",
    "    CUDA_DEVICE = \"cuda:5\"\n",
    "    device = torch.device(CUDA_DEVICE)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(\"device %s in total n_gpu %d distributed training\", device, n_gpu)\n",
    "else:\n",
    "    # bad luck, we are on CPU now!\n",
    "    logger.info(\"gpu is out of the picture, let us use CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicate your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME = \"SST5\"\n",
    "DATA_DIR = \"../../data/dataset/SST5/\"\n",
    "            \n",
    "# \"../../data/uncased_L-12_H-768_A-12/\" is for the default BERT-base pretrain\n",
    "BERT_PATH = \"../../data/uncased_L-12_H-768_A-12/\"\n",
    "MODEL_PATH = \"../../results/\" + TASK_NAME + \"/checkpoint.bin\"\n",
    "EVAL_BATCH_SIZE = 24 # you can tune this down depends on GPU you have.\n",
    "\n",
    "# This loads the task processor for you.\n",
    "processors = {\n",
    "    \"IMDb\":IMDb_Processor,\n",
    "    \"SemEval\":SemEval_Processor,\n",
    "    \"SST5\":SST5_Processor,\n",
    "    \"SST2\":SST2_Processor,\n",
    "    \"SST3\":SST3_Processor,\n",
    "    \"Yelp5\":Yelp5_Processor,\n",
    "    \"Yelp2\":Yelp2_Processor,\n",
    "    \"AdvSA\":AdvSA_Processor\n",
    "}\n",
    "\n",
    "processor = processors[TASK_NAME]()\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/30/2020 01:23:11 - INFO - run_classifier -   model = BERTPretrain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, tokenizer = \\\n",
    "    getModelOptimizerTokenizer(model_type=\"BERTPretrain\",\n",
    "                               vocab_file=BERT_PATH + \"vocab.txt\",\n",
    "                               embed_file=None,\n",
    "                               bert_config_file=BERT_PATH + \"bert_config.json\",\n",
    "                               init_checkpoint=MODEL_PATH,\n",
    "                               label_list=label_list,\n",
    "                               do_lower_case=True,\n",
    "                               # below is not required for eval\n",
    "                               num_train_steps=20,\n",
    "                               learning_rate=2e-5,\n",
    "                               base_learning_rate=2e-5,\n",
    "                               warmup_proportion=0.1,\n",
    "                               init_lrp=True)\n",
    "model = model.to(device) # send the model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 315/2210 [00:00<00:00, 3137.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n",
      "1000\n",
      "guid= test-1000\n",
      "text_a= has all the poignancy of a hallmark card and all the comedy of a gallagher stand up act .\n",
      "text_b= None\n",
      "label= 2\n",
      "2000\n",
      "guid= test-2000\n",
      "text_a= it 's still worth a look .\n",
      "text_b= None\n",
      "label= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2210/2210 [00:00<00:00, 2444.65it/s]\n"
     ]
    }
   ],
   "source": [
    "test_examples = processor.get_test_examples(DATA_DIR)\n",
    "test_features = \\\n",
    "    convert_examples_to_features(\n",
    "        test_examples,\n",
    "        label_list,\n",
    "        512,\n",
    "        tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "all_seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                          all_label_ids, all_seq_len)\n",
    "test_dataloader = DataLoader(test_data, batch_size=EVAL_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call evaluation loop to get accuracy and attribution scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 19/93 [03:08<11:37,  9.42s/it]"
     ]
    }
   ],
   "source": [
    "# we did not exclude gradients, for attribution methods\n",
    "model.eval() # this line will deactivate dropouts\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "pred_logits = []\n",
    "actual = []\n",
    "\n",
    "lrp_scores = []\n",
    "inputs_ids = []\n",
    "seqs_lens = []\n",
    "\n",
    "# we don't need gradient in this case.\n",
    "for step, batch in enumerate(tqdm(test_dataloader, desc=\"Iteration\")):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    input_ids, input_mask, segment_ids, label_ids, seq_lens = batch\n",
    "    # truncate to save space and computing resource\n",
    "    max_seq_lens = max(seq_lens)[0]\n",
    "    input_ids = input_ids[:,:max_seq_lens]\n",
    "    input_mask = input_mask[:,:max_seq_lens]\n",
    "    segment_ids = segment_ids[:,:max_seq_lens]\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    seq_lens = seq_lens.to(device)\n",
    "\n",
    "    # intentially with gradient\n",
    "    tmp_test_loss, logits = \\\n",
    "        model(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                device=device, labels=label_ids)\n",
    "\n",
    "    # for lrp\n",
    "    LRP_class = len(label_list) - 1\n",
    "    Rout_mask = torch.zeros((input_ids.shape[0], len(label_list))).to(device)\n",
    "    Rout_mask[:, LRP_class] = 1.0\n",
    "    relevance_score = logits*Rout_mask\n",
    "    lrp_score = model.backward_lrp(relevance_score).sum(dim=-1).cpu().data\n",
    "    input_ids = input_ids.cpu().data\n",
    "    seq_lens = seq_lens.cpu().data\n",
    "    lrp_scores.append(lrp_score)\n",
    "    inputs_ids.append(input_ids)\n",
    "    seqs_lens.append(seq_lens)\n",
    "    \n",
    "    # for gradient\n",
    "    \n",
    "    # for attention only tracing\n",
    "    \n",
    "    \n",
    "    logits = F.softmax(logits, dim=-1)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    pred_logits.append(logits)\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    actual.append(label_ids)\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    tmp_test_accuracy=np.sum(outputs == label_ids)\n",
    "\n",
    "    test_loss += tmp_test_loss.mean().item()\n",
    "    test_accuracy += tmp_test_accuracy\n",
    "\n",
    "    nb_test_examples += input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "    \n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = collections.OrderedDict()\n",
    "result = {'test_loss': test_loss,\n",
    "            str(len(label_list))+ '-class test_accuracy': test_accuracy}\n",
    "logger.info(\"***** Eval results *****\")\n",
    "for key in result.keys():\n",
    "    logger.info(\"  %s = %s\\n\", key, str(result[key]))\n",
    "# get predictions needed for evaluation\n",
    "pred_logits = np.concatenate(pred_logits, axis=0)\n",
    "actual = np.concatenate(actual, axis=0)\n",
    "pred_label = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "lrp_state_dict = dict()\n",
    "lrp_state_dict[\"lrp_scores\"] = lrp_scores\n",
    "lrp_state_dict[\"inputs_ids\"] = inputs_ids\n",
    "lrp_state_dict[\"seqs_lens\"] = seqs_lens\n",
    "logger.info(\"***** Finish LRP *****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregated lrp scores on a token aggregated across a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lrp_states(task):\n",
    "    lrp_state_dict = torch.load(os.path.join(lrp_data_dir + task + \"/lrp_state.pt\"))\n",
    "    lrp_scores = lrp_state_dict[\"lrp_scores\"]\n",
    "    inputs_ids = lrp_state_dict[\"inputs_ids\"]\n",
    "    seqs_lens = lrp_state_dict[\"seqs_lens\"]\n",
    "    return lrp_scores, inputs_ids, seqs_lens\n",
    "\n",
    "def inverse_mapping(vocab_dict):\n",
    "    inverse_vocab_dict = {}\n",
    "    for k, v in vocab_dict.items():\n",
    "        inverse_vocab_dict[v] = k\n",
    "    return inverse_vocab_dict\n",
    "\n",
    "def translate(token_ids, vocab):\n",
    "    tokens = []\n",
    "    for _id in token_ids.tolist():\n",
    "        tokens.append(vocab[_id])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SST-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp_scores, inputs_ids, seqs_lens = load_lrp_states(\"SST5\")\n",
    "vocab = inverse_mapping(load_vocab(vocab_data_dir, pretrain=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lrp = {}\n",
    "word_lrp_list = []\n",
    "for batch_idx in range(len(inputs_ids)):\n",
    "    for seq_idx in range(inputs_ids[batch_idx].shape[0]):\n",
    "        seq_len = seqs_lens[batch_idx][seq_idx].tolist()[0]\n",
    "        tokens = translate(inputs_ids[batch_idx][seq_idx], vocab)[:seq_len]\n",
    "        lrp_ss = lrp_scores[batch_idx][seq_idx].tolist()[:seq_len]\n",
    "        for i in range(len(tokens)):\n",
    "            word_lrp_list.append((tokens[i], lrp_ss[i]))\n",
    "            if tokens[i] in word_lrp.keys():\n",
    "                word_lrp[tokens[i]].append(lrp_ss[i])\n",
    "            else:\n",
    "                word_lrp[tokens[i]] = [lrp_ss[i]]\n",
    "filter_word_lrp = {}\n",
    "for k, v in word_lrp.items():\n",
    "    if len(v) > 0:\n",
    "        filter_word_lrp[k] = sum(v)*1.0/len(v)\n",
    "filter_word_lrp = [(k, v) for k, v in filter_word_lrp.items()] \n",
    "filter_word_lrp.sort(key = lambda x: x[1], reverse=True)  \n",
    "word_lrp_list.sort(key = lambda x: x[1], reverse=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alright', 1.019275188446045),\n",
       " ('undertaker', 0.8830116987228394),\n",
       " ('nokia', 0.8632627725601196),\n",
       " ('undertaker', 0.8587819933891296),\n",
       " ('wwe', 0.767245888710022),\n",
       " ('buffy', 0.7479455471038818),\n",
       " ('excited', 0.7397792935371399),\n",
       " ('thankful', 0.7372829914093018),\n",
       " ('mccartney', 0.7239643931388855),\n",
       " ('wwe', 0.7167426347732544),\n",
       " ('gonna', 0.6992533206939697),\n",
       " ('excited', 0.6982369422912598),\n",
       " ('nintendo', 0.6770433783531189),\n",
       " ('boyfriend', 0.6717503070831299),\n",
       " ('?', 0.6471765041351318),\n",
       " ('halloween', 0.6394405364990234),\n",
       " ('awesome', 0.6379751563072205),\n",
       " ('week', 0.6373948454856873),\n",
       " ('?', 0.6347154378890991),\n",
       " ('grateful', 0.6337091326713562),\n",
       " ('maiden', 0.6289215087890625),\n",
       " ('colbert', 0.6279644966125488),\n",
       " ('yeah', 0.6249728798866272),\n",
       " ('springsteen', 0.6225460767745972),\n",
       " ('homecoming', 0.6137117147445679),\n",
       " ('rapper', 0.6127239465713501),\n",
       " ('aston', 0.6041433811187744),\n",
       " ('nintendo', 0.6025475859642029),\n",
       " ('rebirth', 0.5997878313064575),\n",
       " ('shawn', 0.5962246656417847),\n",
       " ('maiden', 0.5953845977783203),\n",
       " ('federer', 0.5953329801559448),\n",
       " ('?', 0.5941532254219055),\n",
       " ('happy', 0.5916240811347961),\n",
       " ('disney', 0.5866061449050903),\n",
       " ('mtv', 0.5836845636367798),\n",
       " ('weekend', 0.5808528661727905),\n",
       " ('monopoly', 0.5807051658630371),\n",
       " ('nokia', 0.5805290937423706),\n",
       " ('kendrick', 0.5790697932243347),\n",
       " ('excited', 0.5782206058502197),\n",
       " ('tomorrow', 0.5772755146026611),\n",
       " ('jurassic', 0.5772175788879395),\n",
       " ('maiden', 0.5765378475189209),\n",
       " ('nba', 0.5747071504592896),\n",
       " ('diaries', 0.5741622447967529),\n",
       " ('awesome', 0.5723786354064941),\n",
       " ('nigeria', 0.570912778377533),\n",
       " ('parking', 0.5708843469619751),\n",
       " ('hopefully', 0.5706348419189453),\n",
       " ('tonight', 0.5694618225097656),\n",
       " ('.', 0.568771481513977),\n",
       " ('.', 0.5663012266159058),\n",
       " ('grossing', 0.5660892724990845),\n",
       " ('lyrics', 0.5655071139335632),\n",
       " ('beyonce', 0.5642757415771484),\n",
       " ('federer', 0.562295138835907),\n",
       " ('movies', 0.5614268183708191),\n",
       " ('alright', 0.5586094856262207),\n",
       " ('shortlisted', 0.5579737424850464),\n",
       " ('match', 0.5532442331314087),\n",
       " ('iron', 0.5524681806564331),\n",
       " ('wars', 0.551009476184845),\n",
       " ('gorgeous', 0.5489511489868164),\n",
       " ('grateful', 0.5484281778335571),\n",
       " ('parramatta', 0.5482078194618225),\n",
       " ('rehearsals', 0.5478571057319641),\n",
       " ('nashville', 0.5462385416030884),\n",
       " ('thank', 0.5460221767425537),\n",
       " ('iphone', 0.5455551147460938),\n",
       " ('match', 0.5445926189422607),\n",
       " ('big', 0.5445787310600281),\n",
       " ('tanzania', 0.5438524484634399),\n",
       " ('analysis', 0.543275773525238),\n",
       " ('yeah', 0.5426006317138672),\n",
       " ('.', 0.5385856032371521),\n",
       " ('amazing', 0.5383089184761047),\n",
       " ('nights', 0.5360652208328247),\n",
       " ('baseball', 0.535451352596283),\n",
       " ('espn', 0.5328395962715149),\n",
       " ('derby', 0.530224084854126),\n",
       " ('nba', 0.5290064811706543),\n",
       " ('scriptures', 0.5288156270980835),\n",
       " ('fashion', 0.5285648107528687),\n",
       " ('definitely', 0.5284545421600342),\n",
       " ('excited', 0.5276891589164734),\n",
       " ('thursday', 0.527509331703186),\n",
       " ('emmy', 0.5262451171875),\n",
       " ('concerts', 0.5242246985435486),\n",
       " ('twilight', 0.5218360424041748),\n",
       " ('[SEP]', 0.5201705694198608),\n",
       " ('.', 0.5193117260932922),\n",
       " ('kanye', 0.5174188017845154),\n",
       " ('i', 0.5161038637161255),\n",
       " ('uk', 0.5157416462898254),\n",
       " ('fashion', 0.5142723321914673),\n",
       " ('!', 0.5139952898025513),\n",
       " ('undertaker', 0.5128140449523926),\n",
       " ('nfl', 0.5122625231742859),\n",
       " ('mummy', 0.5122504830360413)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lrp_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('taylor', -0.3367699682712555),\n",
       " ('amanda', -0.3370693325996399),\n",
       " ('tomorrow', -0.33790042996406555),\n",
       " (':', -0.3379462957382202),\n",
       " ('reminds', -0.3385217785835266),\n",
       " ('anything', -0.34000062942504883),\n",
       " ('went', -0.34044402837753296),\n",
       " ('shit', -0.34194329380989075),\n",
       " ('fuck', -0.3427245020866394),\n",
       " ('yankee', -0.3431705832481384),\n",
       " ('.', -0.34351444244384766),\n",
       " ('watching', -0.34353217482566833),\n",
       " ('[CLS]', -0.34413909912109375),\n",
       " ('gonna', -0.3455921411514282),\n",
       " ('thanksgiving', -0.34575310349464417),\n",
       " ('niall', -0.34684664011001587),\n",
       " ('ready', -0.3488250970840454),\n",
       " ('mentally', -0.35005658864974976),\n",
       " ('something', -0.35091954469680786),\n",
       " ('riots', -0.35354745388031006),\n",
       " ('barely', -0.35374513268470764),\n",
       " ('divorce', -0.3550001382827759),\n",
       " ('i', -0.35557347536087036),\n",
       " ('will', -0.3558569550514221),\n",
       " ('my', -0.3571268916130066),\n",
       " ('nba', -0.3614344596862793),\n",
       " ('.', -0.3616555631160736),\n",
       " ('.', -0.36282384395599365),\n",
       " ('.', -0.36341166496276855),\n",
       " ('\\\\', -0.3635162115097046),\n",
       " ('michael', -0.36356526613235474),\n",
       " ('giants', -0.3640194237232208),\n",
       " ('tonight', -0.3676881194114685),\n",
       " ('really', -0.3680019676685333),\n",
       " ('don', -0.36819976568222046),\n",
       " ('bjp', -0.3690427243709564),\n",
       " ('.', -0.3692825734615326),\n",
       " ('album', -0.3699209988117218),\n",
       " ('[CLS]', -0.370639443397522),\n",
       " ('fuck', -0.3711833357810974),\n",
       " ('talked', -0.3717228174209595),\n",
       " ('testified', -0.3721970021724701),\n",
       " ('batman', -0.37591955065727234),\n",
       " ('1st', -0.3830958604812622),\n",
       " ('[SEP]', -0.38654273748397827),\n",
       " ('alright', -0.38927239179611206),\n",
       " ('[SEP]', -0.39883702993392944),\n",
       " ('tomorrow', -0.40438520908355713),\n",
       " ('beth', -0.4063374102115631),\n",
       " ('sox', -0.40704473853111267),\n",
       " ('sox', -0.4101296663284302),\n",
       " ('.', -0.41344600915908813),\n",
       " ('satan', -0.41398885846138),\n",
       " ('(', -0.41505950689315796),\n",
       " ('disney', -0.4169398844242096),\n",
       " ('brian', -0.4203035235404968),\n",
       " ('album', -0.4204477071762085),\n",
       " ('carnival', -0.4221785068511963),\n",
       " ('shankar', -0.42424505949020386),\n",
       " ('album', -0.4245263934135437),\n",
       " ('wwe', -0.4248430132865906),\n",
       " ('amendment', -0.426460325717926),\n",
       " ('i', -0.42729371786117554),\n",
       " ('niall', -0.4281074106693268),\n",
       " ('.', -0.42943644523620605),\n",
       " ('niall', -0.4295881390571594),\n",
       " ('kendrick', -0.4312402009963989),\n",
       " ('[SEP]', -0.433325856924057),\n",
       " ('jackson', -0.4344835579395294),\n",
       " ('tonight', -0.4347270727157593),\n",
       " ('what', -0.4358132779598236),\n",
       " ('[CLS]', -0.43607187271118164),\n",
       " ('detroit', -0.4375481903553009),\n",
       " ('bjp', -0.4396379590034485),\n",
       " ('attributed', -0.4401932656764984),\n",
       " ('suicide', -0.4415339529514313),\n",
       " ('gonna', -0.44216448068618774),\n",
       " ('[SEP]', -0.44245198369026184),\n",
       " ('?', -0.4427078664302826),\n",
       " ('hogan', -0.443478524684906),\n",
       " ('stewart', -0.4453178346157074),\n",
       " ('fuck', -0.44824618101119995),\n",
       " ('dodgers', -0.448966383934021),\n",
       " ('music', -0.4512890577316284),\n",
       " ('gonna', -0.4626166820526123),\n",
       " ('contestant', -0.46337562799453735),\n",
       " ('march', -0.4647156894207001),\n",
       " ('[CLS]', -0.46472686529159546),\n",
       " ('[CLS]', -0.4653722643852234),\n",
       " ('song', -0.46731674671173096),\n",
       " ('.', -0.46796125173568726),\n",
       " ('.', -0.47106507420539856),\n",
       " ('.', -0.4967604875564575),\n",
       " ('video', -0.4982740879058838),\n",
       " ('releases', -0.5037904381752014),\n",
       " ('fuck', -0.5152168273925781),\n",
       " ('album', -0.5165694952011108),\n",
       " ('pga', -0.5199040174484253),\n",
       " ('album', -0.6132038831710815),\n",
       " ('pregnant', -0.9256899356842041)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lrp_list[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('buffy', 0.7479455471038818),\n",
       " ('thankful', 0.7372829914093018),\n",
       " ('aston', 0.6041433811187744),\n",
       " ('rebirth', 0.5997878313064575),\n",
       " ('monopoly', 0.5807051658630371),\n",
       " ('parking', 0.5708843469619751),\n",
       " ('grossing', 0.5660892724990845),\n",
       " ('shortlisted', 0.5579737424850464),\n",
       " ('rehearsals', 0.5478571057319641),\n",
       " ('springsteen', 0.5440821647644043),\n",
       " ('analysis', 0.543275773525238),\n",
       " ('scriptures', 0.5288156270980835),\n",
       " ('concerts', 0.5242246985435486),\n",
       " ('mummy', 0.5122504830360413),\n",
       " ('albums', 0.5100706815719604),\n",
       " ('daytona', 0.49747034907341003),\n",
       " ('happier', 0.49571555852890015),\n",
       " ('sipping', 0.4943842887878418),\n",
       " ('parramatta', 0.49320822954177856),\n",
       " ('buzzing', 0.49006736278533936)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_word_lrp[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('recognise', -0.32276779413223267),\n",
       " ('instinct', -0.3252364993095398),\n",
       " ('bjp', -0.3279317418734233),\n",
       " ('speech', -0.3292480707168579),\n",
       " ('thru', -0.32933974266052246),\n",
       " ('varsity', -0.3318825960159302),\n",
       " ('juventus', -0.3321632742881775),\n",
       " ('ties', -0.33269569277763367),\n",
       " ('reminds', -0.3385217785835266),\n",
       " ('yankee', -0.3431705832481384),\n",
       " ('pga', -0.3462705910205841),\n",
       " ('mentally', -0.35005658864974976),\n",
       " ('barely', -0.35374513268470764),\n",
       " ('shankar', -0.3691314160823822),\n",
       " ('testified', -0.3721970021724701),\n",
       " ('attributed', -0.4401932656764984),\n",
       " ('suicide', -0.4415339529514313),\n",
       " ('pregnant', -0.44782427698373795),\n",
       " ('dodgers', -0.448966383934021),\n",
       " ('contestant', -0.46337562799453735)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_word_lrp[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
