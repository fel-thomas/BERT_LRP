{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run relevance backout here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from util.optimization import BERTAdam\n",
    "from util.processor import *\n",
    "\n",
    "\n",
    "from util.tokenization import *\n",
    "\n",
    "from util.evaluation import *\n",
    "\n",
    "from util.train_helper import *\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# this imports most of the helpers needed to eval the model\n",
    "from run_classifier import *\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import operator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "RETRAIN = False\n",
    "vocab_data_dir = \"../../models/BERT-Google/vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/23/2020 02:26:22 - INFO - run_classifier -   gpu is out of the picture, let us use CPU\n"
     ]
    }
   ],
   "source": [
    "# Note that this notebook only supports single GPU evaluation\n",
    "# which is sufficient for most of tasks by using lower batch size.\n",
    "IS_CUDA = False\n",
    "if IS_CUDA:\n",
    "    CUDA_DEVICE = \"cuda:0\"\n",
    "    device = torch.device(CUDA_DEVICE)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(\"device %s in total n_gpu %d distributed training\", device, n_gpu)\n",
    "else:\n",
    "    # bad luck, we are on CPU now!\n",
    "    logger.info(\"gpu is out of the picture, let us use CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "def inverse_mapping(vocab_dict):\n",
    "    inverse_vocab_dict = {}\n",
    "    for k, v in vocab_dict.items():\n",
    "        inverse_vocab_dict[v] = k\n",
    "    return inverse_vocab_dict\n",
    "\n",
    "def translate(token_ids, vocab):\n",
    "    tokens = []\n",
    "    for _id in token_ids.tolist():\n",
    "        tokens.append(vocab[_id])\n",
    "    return tokens\n",
    "\n",
    "def heatmap_viz(token_grad, vmin=0, vmax=1, cmap=\"Blues\"):\n",
    "    scores = [tu[1] for tu in token_grad]\n",
    "    tokens = [tu[0] for tu in token_grad]\n",
    "    fig, ax = plt.subplots(figsize=(10,1))\n",
    "    ax = sns.heatmap([scores], cmap=cmap, xticklabels=tokens, yticklabels=False,\n",
    "                     cbar_kws=dict(shrink=1, aspect=4, ), linewidths=0.8, vmin=vmin, vmax=vmax)\n",
    "    ax.set_xticklabels(tokens, size = 18)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    # here set the labelsize by 20\n",
    "    cbar.ax.tick_params(labelsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_with_hooks(test_dataloader, model, device, label_list):\n",
    "\n",
    "    # we did not exclude gradients, for attribution methods\n",
    "    model.eval() # this line will deactivate dropouts\n",
    "    test_loss, test_accuracy = 0, 0\n",
    "    nb_test_steps, nb_test_examples = 0, 0\n",
    "    pred_logits = []\n",
    "    actual = []\n",
    "\n",
    "    gs_scores = []\n",
    "    gi_scores = []\n",
    "    lrp_scores = []\n",
    "    lat_scores = []\n",
    "\n",
    "    inputs_ids = []\n",
    "    seqs_lens = []\n",
    "\n",
    "    # we don't need gradient in this case.\n",
    "    for _, batch in enumerate(tqdm(test_dataloader, desc=\"Iteration\")):\n",
    "        input_ids, input_mask, segment_ids, label_ids, seq_lens = batch\n",
    "        # truncate to save space and computing resource\n",
    "        max_seq_lens = max(seq_lens)[0]\n",
    "        input_ids = input_ids[:,:max_seq_lens]\n",
    "        input_mask = input_mask[:,:max_seq_lens]\n",
    "        segment_ids = segment_ids[:,:max_seq_lens]\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        seq_lens = seq_lens.to(device)\n",
    "\n",
    "        # intentially with gradient\n",
    "        tmp_test_loss, logits, all_encoder_attention_scores, embedding_output = \\\n",
    "            model(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                    device=device, labels=label_ids)\n",
    "        logits_raw = F.softmax(logits, dim=-1)\n",
    "\n",
    "        logits = logits_raw.detach().cpu().numpy()\n",
    "        pred_logits.append(logits)\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        actual.append(label_ids)\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "        tmp_test_accuracy=np.sum(outputs == label_ids)\n",
    "        \n",
    "        sensitivity_class = len(label_list) - 1\n",
    "\n",
    "        # GS\n",
    "        gs_score = torch.zeros(logits.shape)\n",
    "        gs_score[:, sensitivity_class] = 1.0\n",
    "        gs_score = model.backward_gradient(gs_score)\n",
    "        gs_score = torch.norm(gs_score, dim=-1)*torch.norm(gs_score, dim=-1)\n",
    "        gs_scores.append(gs_score)\n",
    "        \n",
    "        # GI\n",
    "        gi_score = torch.zeros(logits.shape)\n",
    "        gi_score[:, sensitivity_class] = 1.0\n",
    "        gi_score = model.backward_gradient_input(gi_score)\n",
    "        gi_score = torch.norm(gi_score, dim=-1)*torch.norm(gi_score, dim=-1)\n",
    "        gi_scores.append(gi_score)\n",
    "\n",
    "        \n",
    "        # lrp\n",
    "        Rout_mask = torch.zeros((input_ids.shape[0], len(label_list))).to(device)\n",
    "        Rout_mask[:, sensitivity_class] = 1.0\n",
    "        relevance_score = logits_raw*Rout_mask\n",
    "        lrp_score = model.backward_lrp(relevance_score)\n",
    "        lrp_score = lrp_score.cpu().detach().data\n",
    "        lrp_score = torch.abs(lrp_score).sum(dim=-1)\n",
    "        lrp_scores.append(lrp_score)\n",
    "\n",
    "        # lat\n",
    "        attention_scores = model.backward_lat(input_ids, all_encoder_attention_scores)\n",
    "        lat_scores.append(attention_scores.sum(dim=-1))\n",
    "\n",
    "        # other meta-data\n",
    "        input_ids = input_ids.cpu().data\n",
    "        seq_lens = seq_lens.cpu().data\n",
    "        inputs_ids.append(input_ids)\n",
    "        seqs_lens.append(seq_lens)\n",
    "\n",
    "        test_loss += tmp_test_loss.mean().item()\n",
    "        test_accuracy += tmp_test_accuracy\n",
    "\n",
    "        nb_test_examples += input_ids.size(0)\n",
    "        nb_test_steps += 1\n",
    "\n",
    "    test_loss = test_loss / nb_test_steps\n",
    "    test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "    result = collections.OrderedDict()\n",
    "    result = {'test_loss': test_loss,\n",
    "                str(len(label_list))+ '-class test_accuracy': test_accuracy}\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in result.keys():\n",
    "        logger.info(\"  %s = %s\\n\", key, str(result[key]))\n",
    "    # get predictions needed for evaluation\n",
    "    pred_logits = np.concatenate(pred_logits, axis=0)\n",
    "    actual = np.concatenate(actual, axis=0)\n",
    "    pred_label = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    attribution_scores_state_dict = dict()\n",
    "    attribution_scores_state_dict[\"inputs_ids\"] = inputs_ids\n",
    "    attribution_scores_state_dict[\"seqs_lens\"] = seqs_lens\n",
    "    attribution_scores_state_dict[\"gs_scores\"] = gs_scores\n",
    "    attribution_scores_state_dict[\"gi_scores\"] = gi_scores\n",
    "    attribution_scores_state_dict[\"lrp_scores\"] = lrp_scores\n",
    "    attribution_scores_state_dict[\"lat_scores\"] = lat_scores\n",
    "\n",
    "    logger.info(\"***** Finish Attribution Backouts *****\")\n",
    "    return attribution_scores_state_dict\n",
    "\n",
    "def analysis_task(task_name, device, sentence_limit=5000):\n",
    "    \"\"\"\n",
    "    We need to set a limit otherwise it takes too long!\n",
    "    \"\"\"\n",
    "    TASK_NAME = task_name\n",
    "    lrp_data_dir = \"../../results\"\n",
    "    vocab_data_dir = \"../../models/BERT-Google/vocab.txt\"\n",
    "    DATA_DIR = \"../../datasets/\" + TASK_NAME + \"/\"\n",
    "\n",
    "    # \"../../data/uncased_L-12_H-768_A-12/\" is for the default BERT-base pretrain\n",
    "    BERT_PATH = \"../../models/BERT-Google/\"\n",
    "    MODEL_PATH = \"../../results/\" + TASK_NAME + \"/best_checkpoint.bin\"\n",
    "    EVAL_BATCH_SIZE = 24 # you can tune this down depends on GPU you have.\n",
    "\n",
    "    # This loads the task processor for you.\n",
    "    processors = {\n",
    "        \"SST5\": SST5_Processor,\n",
    "        \"SemEval\" : SemEval_Processor,\n",
    "        \"IMDb\" : IMDb_Processor,\n",
    "        \"Yelp5\" : Yelp5_Processor\n",
    "    }\n",
    "\n",
    "    processor = processors[TASK_NAME]()\n",
    "    label_list = processor.get_labels()\n",
    "    \n",
    "    model, tokenizer, optimizer = \\\n",
    "        load_model_setups(vocab_file=BERT_PATH + \"vocab.txt\",\n",
    "                           bert_config_file=BERT_PATH + \"bert_config.json\",\n",
    "                           init_checkpoint=MODEL_PATH,\n",
    "                           label_list=label_list,\n",
    "                           num_train_steps=20,\n",
    "                           do_lower_case=True,\n",
    "                           # below is not required for eval\n",
    "                           learning_rate=2e-5,\n",
    "                           warmup_proportion=0.1,\n",
    "                           init_lrp=True)\n",
    "    model = model.to(device) # send the model to device\n",
    "    \n",
    "    test_examples = processor.get_test_examples(DATA_DIR, sentence_limit=sentence_limit)\n",
    "    test_features = \\\n",
    "        convert_examples_to_features(\n",
    "            test_examples,\n",
    "            label_list,\n",
    "            128,\n",
    "            tokenizer)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "    all_seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "\n",
    "    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                              all_label_ids, all_seq_len)\n",
    "\n",
    "    test_dataloader = DataLoader(test_data, batch_size=EVAL_BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    score_dict = evaluate_with_hooks(test_dataloader, model, device, label_list)\n",
    "    \n",
    "    return score_dict\n",
    "\n",
    "def find_common_vocab(dict_list):\n",
    "    assert len(dict_list) > 0\n",
    "    common_vocab = set(dict_list[0].keys())\n",
    "    for i in range(1, len(dict_list)):\n",
    "        common_vocab = common_vocab.intersection(set(dict_list[i].keys()))\n",
    "    return common_vocab\n",
    "\n",
    "def subset_score(dict_list):\n",
    "    common_vocab = find_common_vocab(dict_list)\n",
    "    per_word_score = []\n",
    "    for word in common_vocab:\n",
    "        word_score = []\n",
    "        for d in dict_list:\n",
    "            word_score.append(d[word])\n",
    "        per_word_score.append(word_score)\n",
    "    return np.transpose(np.array(per_word_score)) \n",
    "\n",
    "def load_attribution_scores(vocab_data_dir, inputs_ids, seqs_lens, raw_attribution_scores, min_freq=1, \n",
    "                            consider_speicial_tokens=False, normalized=True, min_length=0):\n",
    "    vocab = inverse_mapping(load_vocab(vocab_data_dir, pretrain=False))\n",
    "    word_lrp = {}\n",
    "    word_lrp_list = []\n",
    "    sentence_lrp = []\n",
    "    for batch_idx in range(len(inputs_ids)):\n",
    "        for seq_idx in range(inputs_ids[batch_idx].shape[0]):\n",
    "            seq_len = seqs_lens[batch_idx][seq_idx].tolist()[0]\n",
    "            if consider_speicial_tokens:\n",
    "                tokens = translate(inputs_ids[batch_idx][seq_idx], vocab)[:seq_len]\n",
    "                attribution_scores = raw_attribution_scores[batch_idx][seq_idx][:seq_len]\n",
    "            else:\n",
    "                tokens = translate(inputs_ids[batch_idx][seq_idx], vocab)[:seq_len][1:-1]\n",
    "                attribution_scores = raw_attribution_scores[batch_idx][seq_idx][:seq_len][1:-1] \n",
    "            if normalized:\n",
    "                sentence_attribution_scores = F.softmax(torch.abs(attribution_scores), dim=-1).tolist()\n",
    "            else:\n",
    "                sentence_attribution_scores = attribution_scores.tolist()\n",
    "            if len(tokens) >= min_length:\n",
    "                assert(len(tokens) == len(sentence_attribution_scores))\n",
    "                s_lrp = list(zip(tokens, sentence_attribution_scores))\n",
    "                sentence_lrp.append(s_lrp)\n",
    "                for i in range(len(s_lrp)):\n",
    "                    token = s_lrp[i][0]\n",
    "                    score = s_lrp[i][1]\n",
    "                    word_lrp_list.append((token, score))\n",
    "                    if token in word_lrp.keys():\n",
    "                        word_lrp[token].append(score)\n",
    "                    else:\n",
    "                        word_lrp[token] = [score]\n",
    "\n",
    "    filter_word_lrp = {}\n",
    "    for k, v in word_lrp.items():\n",
    "        if len(v) > min_freq:\n",
    "            filter_word_lrp[k] = sum(v)*1.0/len(v)\n",
    "    filter_word_lrp = [(k, v) for k, v in filter_word_lrp.items()] \n",
    "    filter_word_lrp.sort(key = lambda x: x[1], reverse=True)  \n",
    "    word_lrp_list.sort(key = lambda x: x[1], reverse=True)\n",
    "    return filter_word_lrp, word_lrp_list, sentence_lrp\n",
    "\n",
    "def load_attribution_meta(vocab_data_dir, dataset_dict):\n",
    "    attribution_meta = {}\n",
    "    for item in [\"gs_scores\", \"gi_scores\", \\\n",
    "                 \"lrp_scores\", \"lat_scores\"]:\n",
    "        filtered_word_rank, raw_word_rank, sentence_revelance_score = \\\n",
    "            load_attribution_scores(vocab_data_dir,\n",
    "                                    dataset_dict[\"inputs_ids\"], \n",
    "                                    dataset_dict[\"seqs_lens\"],\n",
    "                                    dataset_dict[item])\n",
    "        attribution_meta[item] = {\"filtered_word_rank\": filtered_word_rank, \n",
    "                                  \"raw_word_rank\": raw_word_rank, \n",
    "                                  \"sentence_revelance_score\": sentence_revelance_score}\n",
    "    return attribution_meta\n",
    "\n",
    "def print_topk_words(attribution_meta, k=30, filtered=True):\n",
    "    \"\"\"\n",
    "    print top k words for a dataset\n",
    "    \"\"\"\n",
    "    from tabulate import tabulate\n",
    "    words = []\n",
    "    words_neg = []\n",
    "    index = 0\n",
    "    for i in range(0, k):\n",
    "        item_words = []\n",
    "        item_words_neg = []\n",
    "        for item in [\"gs_scores\", \"gi_scores\", \\\n",
    "                     \"lrp_scores\", \"lat_scores\"]:\n",
    "            \n",
    "            word_rank = None\n",
    "            if filtered:\n",
    "                word_rank = attribution_meta[item][\"filtered_word_rank\"]\n",
    "            else:\n",
    "                word_rank = attribution_meta[item][\"raw_word_rank\"]\n",
    "            item_words.append((word_rank[i][0], str(word_rank[i][1])[:4]))\n",
    "            item_words_neg.append((word_rank[-(i+1)][0], str(word_rank[-(i+1)][1])[:4]))\n",
    "\n",
    "        words.append(item_words)\n",
    "        words_neg.append(item_words_neg) # reversed ranking\n",
    "\n",
    "    print(tabulate(words, headers=[\"gs_scores\", \"gi_scores\", \"lrp_scores\", \"lat_scores\"]))\n",
    "    print(\"***\")\n",
    "    print(tabulate(words_neg, headers=[\"gs_scores\", \"gi_scores\", \"lrp_scores\", \"lat_scores\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3.2.1 SST-5 Word Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst5_dict = analysis_task(\"SST5\", device, sentence_limit=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst5_attribution_meta = load_attribution_meta(vocab_data_dir, sst5_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topk_words(sst5_attribution_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap_viz(sentence_lrps[i], vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp. 3.2.2 Word deletion experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,3)\n",
    "a[[]].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "def random_drop(input_ids, seq_lens, k=1):\n",
    "    for b in range(input_ids.shape[0]):\n",
    "        if k > seq_lens[b][0]:\n",
    "            input_ids[b] = 0. # zero out all of them\n",
    "        else:\n",
    "            zero_out_idx = random.sample(range(1, seq_lens[b][0]), k)\n",
    "            for idx in zero_out_idx:\n",
    "                input_ids[b][idx] = 0.\n",
    "    return input_ids\n",
    "\n",
    "def topk_drop(input_ids, scores, k=1):\n",
    "    if k > input_ids.shape[1]-2:\n",
    "        input_ids = 0.\n",
    "    else:\n",
    "        _, idx = torch.topk(scores[:,1:-1], k, dim=-1)\n",
    "        idx = idx + 1\n",
    "        for b in range(input_ids.shape[0]):\n",
    "            input_ids[b, idx[b]] = 0.\n",
    "    return input_ids\n",
    "\n",
    "def evaluate_with_word_deletion(test_dataloader, model, device, label_list, \n",
    "                                k=0, del_type=\"gi\", \n",
    "                                original_correct=True):\n",
    "\n",
    "    # we did not exclude gradients, for attribution methods\n",
    "    model.eval() # this line will deactivate dropouts\n",
    "    test_loss, test_accuracy = 0, 0\n",
    "    nb_test_steps, nb_test_examples = 0, 0\n",
    "    pred_logits = []\n",
    "    actual = []\n",
    "\n",
    "    inputs_ids = []\n",
    "    seqs_lens = []\n",
    "\n",
    "    # we don't need gradient in this case.\n",
    "    for _, batch in enumerate(tqdm(test_dataloader, desc=\"Iteration\")):\n",
    "        input_ids, input_mask, segment_ids, label_ids, seq_lens = batch\n",
    "        # truncate to save space and computing resource\n",
    "        max_seq_lens = max(seq_lens)[0]\n",
    "        input_ids = input_ids[:,:max_seq_lens]\n",
    "        input_mask = input_mask[:,:max_seq_lens]\n",
    "        segment_ids = segment_ids[:,:max_seq_lens]\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "        seq_lens = seq_lens.to(device)\n",
    "        \n",
    "        sensitivity_class = len(label_list) - 1\n",
    "        \n",
    "        tmp_test_loss, logits, all_encoder_attention_scores, embedding_output = \\\n",
    "            model(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                    device=device, labels=label_ids)\n",
    "        logits_raw = F.softmax(logits, dim=-1)\n",
    "        logits = logits_raw.detach().cpu().numpy()\n",
    "        label_ids_raw = label_ids.to('cpu').numpy()\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "        tmp_idx_correct = outputs == label_ids_raw\n",
    "        tmp_idx_correct = tmp_idx_correct.nonzero()[0]\n",
    "        tmp_idx_wrong = outputs != label_ids_raw\n",
    "        tmp_idx_wrong = tmp_idx_wrong.nonzero()[0]\n",
    "\n",
    "        if original_correct:\n",
    "            # select only those that correct\n",
    "            new_input_ids = input_ids[tmp_idx_correct]\n",
    "            new_segment_ids = segment_ids[tmp_idx_correct]\n",
    "            new_input_mask = input_mask[tmp_idx_correct]\n",
    "            new_seq_lens = seq_lens[tmp_idx_correct]\n",
    "            new_label_ids = label_ids[tmp_idx_correct]\n",
    "        else:\n",
    "            # select only those that are wrong\n",
    "            new_input_ids = input_ids[tmp_idx_wrong]\n",
    "            new_segment_ids = segment_ids[tmp_idx_wrong]\n",
    "            new_input_mask = input_mask[tmp_idx_wrong]\n",
    "            new_seq_lens = seq_lens[tmp_idx_wrong]\n",
    "            new_label_ids = label_ids[tmp_idx_wrong]\n",
    "            \n",
    "        # corner case handling, if this batch contains no examples, we bypass\n",
    "        if new_input_ids.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        if k == 0: # no need to drop\n",
    "            tmp_test_loss, logits, all_encoder_attention_scores, embedding_output = \\\n",
    "                model(new_input_ids, new_segment_ids, new_input_mask, new_seq_lens,\n",
    "                        device=device, labels=new_label_ids)\n",
    "        else:\n",
    "            if del_type == \"random\":\n",
    "                # Random dropouts\n",
    "                new_input_ids = random_drop(new_input_ids, new_seq_lens, k=k)\n",
    "                tmp_test_loss, logits, all_encoder_attention_scores, embedding_output = \\\n",
    "                    model(new_input_ids, new_segment_ids, new_input_mask, new_seq_lens,\n",
    "                            device=device, labels=new_label_ids)\n",
    "            elif del_type == \"gs\":\n",
    "                # GS dropouts\n",
    "                gs_score = torch.zeros(logits.shape)\n",
    "                gs_score[:, sensitivity_class] = 1.0\n",
    "                gs_score = model.backward_gradient(gs_score)\n",
    "                gs_score = torch.norm(gs_score, dim=-1)*torch.norm(gs_score, dim=-1)\n",
    "                if original_correct:\n",
    "                    new_gs_score = gs_score[tmp_idx_correct]\n",
    "                else:\n",
    "                    new_gs_score = gs_score[tmp_idx_wrong]\n",
    "                # rerun\n",
    "                new_input_ids = topk_drop(new_input_ids, new_gs_score, k=k)\n",
    "                tmp_test_loss, logits, all_encoder_attention_scores, embedding_output = \\\n",
    "                    model(new_input_ids, new_segment_ids, new_input_mask, new_seq_lens,\n",
    "                            device=device, labels=new_label_ids)\n",
    "            elif del_type == \"gi\":\n",
    "                # GI dropouts\n",
    "                gi_score = torch.zeros(logits.shape)\n",
    "                gi_score[:, sensitivity_class] = 1.0\n",
    "                gi_score = model.backward_gradient_input(gi_score)\n",
    "                gi_score = torch.norm(gi_score, dim=-1)*torch.norm(gi_score, dim=-1)\n",
    "                if original_correct:\n",
    "                    new_gi_score = gi_score[tmp_idx_correct]\n",
    "                else:\n",
    "                    new_gi_score = gi_score[tmp_idx_wrong]\n",
    "                # rerun\n",
    "                new_input_ids = topk_drop(new_input_ids, new_gi_score, k=k)\n",
    "                tmp_test_loss, logits, all_encoder_attention_scores, embedding_output = \\\n",
    "                    model(new_input_ids, new_segment_ids, new_input_mask, new_seq_lens,\n",
    "                            device=device, labels=new_label_ids)\n",
    "            elif del_type == \"lrp\":\n",
    "                # lrp dropouts\n",
    "                Rout_mask = torch.zeros((input_ids.shape[0], len(label_list))).to(device)\n",
    "                Rout_mask[:, sensitivity_class] = 1.0\n",
    "                relevance_score = logits_raw*Rout_mask\n",
    "                lrp_score = model.backward_lrp(relevance_score)\n",
    "                lrp_score = lrp_score.cpu().detach().data\n",
    "                lrp_score = torch.abs(lrp_score).sum(dim=-1)\n",
    "                if original_correct:\n",
    "                    new_lrp_score = lrp_score[tmp_idx_correct]\n",
    "                else:\n",
    "                    new_lrp_score = lrp_score[tmp_idx_wrong]\n",
    "                # rerun\n",
    "                new_input_ids = topk_drop(new_input_ids, new_lrp_score, k=k)\n",
    "                tmp_test_loss, logits, all_encoder_attention_scores, embedding_output = \\\n",
    "                    model(new_input_ids, new_segment_ids, new_input_mask, new_seq_lens,\n",
    "                            device=device, labels=new_label_ids)\n",
    "            elif del_type == \"lat\":\n",
    "                # lat dropouts\n",
    "                attention_scores = model.backward_lat(input_ids, all_encoder_attention_scores)\n",
    "                attention_scores = attention_scores.sum(dim=-1)\n",
    "                if original_correct:\n",
    "                    new_attention_scores = attention_scores[tmp_idx_correct]\n",
    "                else:\n",
    "                    new_attention_scores = attention_scores[tmp_idx_wrong]\n",
    "                # rerun\n",
    "                new_input_ids = topk_drop(new_input_ids, new_attention_scores, k=k)\n",
    "                tmp_test_loss, logits, all_encoder_attention_scores, embedding_output = \\\n",
    "                    model(new_input_ids, new_segment_ids, new_input_mask, new_seq_lens,\n",
    "                            device=device, labels=new_label_ids)\n",
    "\n",
    "        logits_raw = F.softmax(logits, dim=-1)\n",
    "        logits = logits_raw.detach().cpu().numpy()\n",
    "        new_label_ids = new_label_ids.to('cpu').numpy()\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "        tmp_test_accuracy=np.sum(outputs == new_label_ids)\n",
    "\n",
    "        test_loss += tmp_test_loss.mean().item()\n",
    "        test_accuracy += tmp_test_accuracy\n",
    "\n",
    "        nb_test_examples += new_input_ids.size(0)\n",
    "        nb_test_steps += 1\n",
    "\n",
    "    test_loss = test_loss / nb_test_steps\n",
    "    test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "    result = collections.OrderedDict()\n",
    "    result = {'test_loss': test_loss,\n",
    "                str(len(label_list))+ '-class test_accuracy': test_accuracy}\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in result.keys():\n",
    "        logger.info(\"  %s = %s\\n\", key, str(result[key]))\n",
    "\n",
    "def word_deletion_task(task_name, device, sentence_limit=2000, \n",
    "                       k=0, del_type=\"random\",\n",
    "                       original_correct=True):\n",
    "    \"\"\"\n",
    "    We need to set a limit otherwise it takes too long!\n",
    "    \"\"\"\n",
    "    TASK_NAME = task_name\n",
    "    lrp_data_dir = \"../../results\"\n",
    "    vocab_data_dir = \"../../models/BERT-Google/vocab.txt\"\n",
    "    DATA_DIR = \"../../datasets/\" + TASK_NAME + \"/\"\n",
    "\n",
    "    # \"../../data/uncased_L-12_H-768_A-12/\" is for the default BERT-base pretrain\n",
    "    BERT_PATH = \"../../models/BERT-Google/\"\n",
    "    MODEL_PATH = \"../../results/\" + TASK_NAME + \"/best_checkpoint.bin\"\n",
    "    EVAL_BATCH_SIZE = 24 # you can tune this down depends on GPU you have.\n",
    "\n",
    "    # This loads the task processor for you.\n",
    "    processors = {\n",
    "        \"SST5\": SST5_Processor,\n",
    "        \"SemEval\" : SemEval_Processor,\n",
    "        \"IMDb\" : IMDb_Processor,\n",
    "        \"Yelp5\" : Yelp5_Processor\n",
    "    }\n",
    "\n",
    "    processor = processors[TASK_NAME]()\n",
    "    label_list = processor.get_labels()\n",
    "    \n",
    "    model, tokenizer, optimizer = \\\n",
    "        load_model_setups(vocab_file=BERT_PATH + \"vocab.txt\",\n",
    "                           bert_config_file=BERT_PATH + \"bert_config.json\",\n",
    "                           init_checkpoint=MODEL_PATH,\n",
    "                           label_list=label_list,\n",
    "                           num_train_steps=20,\n",
    "                           do_lower_case=True,\n",
    "                           # below is not required for eval\n",
    "                           learning_rate=2e-5,\n",
    "                           warmup_proportion=0.1,\n",
    "                           init_lrp=True)\n",
    "    model = model.to(device) # send the model to device\n",
    "    \n",
    "    test_examples = processor.get_test_examples(DATA_DIR, sentence_limit=sentence_limit)\n",
    "    test_features = \\\n",
    "        convert_examples_to_features(\n",
    "            test_examples,\n",
    "            label_list,\n",
    "            128,\n",
    "            tokenizer)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "    all_seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "\n",
    "    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                              all_label_ids, all_seq_len)\n",
    "\n",
    "    test_dataloader = DataLoader(test_data, batch_size=EVAL_BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    evaluate_with_word_deletion(test_dataloader, model, device, label_list, \n",
    "                                k=k, del_type=del_type, \n",
    "                                original_correct=original_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/23/2020 11:55:02 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:55:02 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:55:02 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=0 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 430/2001 [00:00<00:00, 4290.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4170.91it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:30<00:00,  2.72it/s]\n",
      "12/23/2020 11:55:35 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 11:55:35 - INFO - run_classifier -     test_loss = 0.3908445890105906\n",
      "\n",
      "12/23/2020 11:55:35 - INFO - run_classifier -     5-class test_accuracy = 1.0\n",
      "\n",
      "12/23/2020 11:55:35 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:55:35 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:55:35 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=1 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 458/2001 [00:00<00:00, 4579.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 3682.43it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:31<00:00,  2.64it/s]\n",
      "12/23/2020 11:56:08 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 11:56:08 - INFO - run_classifier -     test_loss = 0.4975708861436163\n",
      "\n",
      "12/23/2020 11:56:08 - INFO - run_classifier -     5-class test_accuracy = 0.8979057591623036\n",
      "\n",
      "12/23/2020 11:56:08 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:56:08 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:56:08 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=2 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 451/2001 [00:00<00:00, 4503.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4096.77it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:30<00:00,  2.73it/s]\n",
      "12/23/2020 11:56:41 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 11:56:41 - INFO - run_classifier -     test_loss = 0.6247674389964059\n",
      "\n",
      "12/23/2020 11:56:41 - INFO - run_classifier -     5-class test_accuracy = 0.8298429319371727\n",
      "\n",
      "12/23/2020 11:56:41 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:56:41 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:56:41 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=3 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 422/2001 [00:00<00:00, 4213.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4139.57it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:30<00:00,  2.73it/s]\n",
      "12/23/2020 11:57:14 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 11:57:14 - INFO - run_classifier -     test_loss = 0.7756362706422806\n",
      "\n",
      "12/23/2020 11:57:14 - INFO - run_classifier -     5-class test_accuracy = 0.7609075043630017\n",
      "\n",
      "12/23/2020 11:57:14 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:57:14 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:57:14 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=4 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 400/2001 [00:00<00:00, 3999.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 3723.45it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:31<00:00,  2.70it/s]\n",
      "12/23/2020 11:57:47 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 11:57:47 - INFO - run_classifier -     test_loss = 0.9476450390758968\n",
      "\n",
      "12/23/2020 11:57:47 - INFO - run_classifier -     5-class test_accuracy = 0.6535776614310645\n",
      "\n",
      "12/23/2020 11:57:47 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:57:47 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:57:47 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=5 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 448/2001 [00:00<00:00, 4477.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4319.54it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:30<00:00,  2.73it/s]\n",
      "12/23/2020 11:58:20 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 11:58:20 - INFO - run_classifier -     test_loss = 1.2040626619543349\n",
      "\n",
      "12/23/2020 11:58:20 - INFO - run_classifier -     5-class test_accuracy = 0.5959860383944153\n",
      "\n",
      "12/23/2020 11:58:20 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:58:20 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:58:20 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=6 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 387/2001 [00:00<00:00, 3862.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4009.68it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:30<00:00,  2.74it/s]\n",
      "12/23/2020 11:58:53 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 11:58:53 - INFO - run_classifier -     test_loss = 1.34529063247499\n",
      "\n",
      "12/23/2020 11:58:53 - INFO - run_classifier -     5-class test_accuracy = 0.5680628272251309\n",
      "\n",
      "12/23/2020 11:58:53 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:58:53 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:58:53 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=7 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 445/2001 [00:00<00:00, 4440.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4502.43it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:30<00:00,  2.74it/s]\n",
      "12/23/2020 11:59:25 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 11:59:25 - INFO - run_classifier -     test_loss = 1.5258239699261529\n",
      "\n",
      "12/23/2020 11:59:25 - INFO - run_classifier -     5-class test_accuracy = 0.5104712041884817\n",
      "\n",
      "12/23/2020 11:59:25 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:59:25 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:59:25 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=8 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 413/2001 [00:00<00:00, 4124.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4145.70it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:31<00:00,  2.68it/s]\n",
      "12/23/2020 11:59:59 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 11:59:59 - INFO - run_classifier -     test_loss = 1.6759203381481624\n",
      "\n",
      "12/23/2020 11:59:59 - INFO - run_classifier -     5-class test_accuracy = 0.4668411867364747\n",
      "\n",
      "12/23/2020 11:59:59 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 11:59:59 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 11:59:59 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=9 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 404/2001 [00:00<00:00, 4035.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4251.21it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:31<00:00,  2.69it/s]\n",
      "12/23/2020 12:00:32 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 12:00:32 - INFO - run_classifier -     test_loss = 1.9307754124913896\n",
      "\n",
      "12/23/2020 12:00:32 - INFO - run_classifier -     5-class test_accuracy = 0.4223385689354276\n",
      "\n",
      "12/23/2020 12:00:32 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 12:00:32 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 12:00:32 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=10 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 413/2001 [00:00<00:00, 4127.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 3754.97it/s]\n",
      "Iteration:  65%|██████▌   | 55/84 [00:20<00:10,  2.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-73bf838e38c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     word_deletion_task(\"SST5\", device, sentence_limit=2000, \n\u001b[1;32m     10\u001b[0m                        \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                        original_correct=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-5db66398c58f>\u001b[0m in \u001b[0;36mword_deletion_task\u001b[0;34m(task_name, device, sentence_limit, k, del_type, original_correct)\u001b[0m\n\u001b[1;32m    224\u001b[0m     evaluate_with_word_deletion(test_dataloader, model, device, label_list, \n\u001b[1;32m    225\u001b[0m                                 \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                                 original_correct=original_correct)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-5db66398c58f>\u001b[0m in \u001b[0;36mevaluate_with_word_deletion\u001b[0;34m(test_dataloader, model, device, label_list, k, del_type, original_correct)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mtmp_test_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_encoder_attention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     model(new_input_ids, new_segment_ids, new_input_mask, new_seq_lens,\n\u001b[0;32m---> 94\u001b[0;31m                             device=device, labels=new_label_ids)\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gs\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;31m# GS dropouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/BERT_LRP/code/model/BERT.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, seq_lens, device, labels)\u001b[0m\n\u001b[1;32m    675\u001b[0m     def forward(self, input_ids, token_type_ids, attention_mask, seq_lens,\n\u001b[1;32m    676\u001b[0m                 device=None, labels=None):\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_encoder_attention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/BERT_LRP/code/model/BERT.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_encoder_attention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional_attn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/BERT_LRP/code/model/BERT.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mall_encoder_attention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m             \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mall_encoder_attention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/BERT_LRP/code/model/BERT.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/BERT_LRP/code/model/BERT.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/BERT_LRP/code/model/BERT.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.044715\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \"\"\"\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# processors = {\n",
    "#     \"SST5\": SST5_Processor,\n",
    "#     \"SemEval\" : SemEval_Processor,\n",
    "#     \"IMDb\" : IMDb_Processor,\n",
    "#     \"Yelp5\" : Yelp5_Processor\n",
    "# }\n",
    "for i in range(1, 6):\n",
    "    print(\"===== Word Deletion with K=%s =====\"%(i))\n",
    "    word_deletion_task(\"SST5\", device, sentence_limit=2000, \n",
    "                       k=i, del_type=\"random\", \n",
    "                       original_correct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/23/2020 16:55:40 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:55:40 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:55:40 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=1 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 130/2001 [00:00<00:01, 1092.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 3595.54it/s]\n",
      "Iteration: 100%|██████████| 84/84 [01:00<00:00,  1.38it/s]\n",
      "12/23/2020 16:56:43 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:56:43 - INFO - run_classifier -     test_loss = 0.9258443450643903\n",
      "\n",
      "12/23/2020 16:56:43 - INFO - run_classifier -     5-class test_accuracy = 0.6928446771378709\n",
      "\n",
      "12/23/2020 16:56:43 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:56:43 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:56:43 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=2 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 432/2001 [00:00<00:00, 4314.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4323.90it/s]\n",
      "Iteration: 100%|██████████| 84/84 [01:02<00:00,  1.35it/s]\n",
      "12/23/2020 16:57:47 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:57:47 - INFO - run_classifier -     test_loss = 1.219329908490181\n",
      "\n",
      "12/23/2020 16:57:47 - INFO - run_classifier -     5-class test_accuracy = 0.606457242582897\n",
      "\n",
      "12/23/2020 16:57:47 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:57:47 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:57:47 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=3 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 463/2001 [00:00<00:00, 4621.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4447.84it/s]\n",
      "Iteration:  12%|█▏        | 10/84 [00:07<00:58,  1.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-03527abfaf71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     word_deletion_task(\"SST5\", device, sentence_limit=2000, \n\u001b[1;32m      4\u001b[0m                        \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                        original_correct=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-c311855308ce>\u001b[0m in \u001b[0;36mword_deletion_task\u001b[0;34m(task_name, device, sentence_limit, k, del_type, original_correct)\u001b[0m\n\u001b[1;32m    238\u001b[0m     evaluate_with_word_deletion(test_dataloader, model, device, label_list, \n\u001b[1;32m    239\u001b[0m                                 \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                                 original_correct=original_correct)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-c311855308ce>\u001b[0m in \u001b[0;36mevaluate_with_word_deletion\u001b[0;34m(test_dataloader, model, device, label_list, k, del_type, original_correct)\u001b[0m\n\u001b[1;32m     56\u001b[0m                     device=device, labels=label_ids)\n\u001b[1;32m     57\u001b[0m         \u001b[0mlogits_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mlabel_ids_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    print(\"===== Word Deletion with K=%s =====\"%(i))\n",
    "    word_deletion_task(\"SST5\", device, sentence_limit=2000, \n",
    "                       k=i, del_type=\"gs\", \n",
    "                       original_correct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/23/2020 12:21:53 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 12:21:53 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 12:21:53 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=1 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 420/2001 [00:00<00:00, 4199.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 3985.35it/s]\n",
      "Iteration:   0%|          | 0/84 [00:00<?, ?it/s]/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:162: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "Iteration: 100%|██████████| 84/84 [01:07<00:00,  1.24it/s]\n",
      "12/23/2020 12:23:03 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 12:23:03 - INFO - run_classifier -     test_loss = 1.4514927438327245\n",
      "\n",
      "12/23/2020 12:23:03 - INFO - run_classifier -     5-class test_accuracy = 0.0\n",
      "\n",
      "12/23/2020 12:23:03 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 12:23:03 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 12:23:03 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=2 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 436/2001 [00:00<00:00, 4350.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4236.37it/s]\n",
      "Iteration:   5%|▍         | 4/84 [00:03<01:19,  1.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-f4b6a98157b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     word_deletion_task(\"SST5\", device, sentence_limit=2000, \n\u001b[1;32m      4\u001b[0m                        \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                        original_correct=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-528d3ae042a8>\u001b[0m in \u001b[0;36mword_deletion_task\u001b[0;34m(task_name, device, sentence_limit, k, del_type, original_correct)\u001b[0m\n\u001b[1;32m    239\u001b[0m     evaluate_with_word_deletion(test_dataloader, model, device, label_list, \n\u001b[1;32m    240\u001b[0m                                 \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                                 original_correct=original_correct)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-528d3ae042a8>\u001b[0m in \u001b[0;36mevaluate_with_word_deletion\u001b[0;34m(test_dataloader, model, device, label_list, k, del_type, original_correct)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mgi_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mgi_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitivity_class\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mgi_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_gradient_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgi_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0mgi_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgi_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgi_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;31m# rerun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/workspace/BERT_LRP/code/model/BERT.py\u001b[0m in \u001b[0;36mbackward_gradient_input\u001b[0;34m(self, sensitivity_grads)\u001b[0m\n\u001b[1;32m    699\u001b[0m         sensitivity_grads = torch.autograd.grad(classifier_out, embedding_output, \n\u001b[1;32m    700\u001b[0m                                                 \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msensitivity_grads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m                                                 retain_graph=True)[0]\n\u001b[0m\u001b[1;32m    702\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msensitivity_grads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membedding_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/user/wuzhengx/tool-chain/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    190\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    191\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    print(\"===== Word Deletion with K=%s =====\"%(i))\n",
    "    word_deletion_task(\"SST5\", device, sentence_limit=2000, \n",
    "                       k=i, del_type=\"gi\", \n",
    "                       original_correct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/23/2020 16:33:17 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:33:17 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:33:17 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=1 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 424/2001 [00:00<00:00, 4237.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4412.77it/s]\n",
      "Iteration: 100%|██████████| 84/84 [01:35<00:00,  1.14s/it]\n",
      "12/23/2020 16:34:55 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:34:55 - INFO - run_classifier -     test_loss = 0.6154856820191655\n",
      "\n",
      "12/23/2020 16:34:55 - INFO - run_classifier -     5-class test_accuracy = 0.8534031413612565\n",
      "\n",
      "12/23/2020 16:34:55 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:34:55 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:34:55 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=2 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 424/2001 [00:00<00:00, 4237.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4422.32it/s]\n",
      "Iteration: 100%|██████████| 84/84 [01:35<00:00,  1.14s/it]\n",
      "12/23/2020 16:36:32 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:36:32 - INFO - run_classifier -     test_loss = 0.8014448458949724\n",
      "\n",
      "12/23/2020 16:36:32 - INFO - run_classifier -     5-class test_accuracy = 0.7486910994764397\n",
      "\n",
      "12/23/2020 16:36:32 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:36:32 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:36:32 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=3 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 451/2001 [00:00<00:00, 4506.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4023.91it/s]\n",
      "Iteration: 100%|██████████| 84/84 [01:33<00:00,  1.12s/it]\n",
      "12/23/2020 16:38:08 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:38:08 - INFO - run_classifier -     test_loss = 1.0009076559827441\n",
      "\n",
      "12/23/2020 16:38:08 - INFO - run_classifier -     5-class test_accuracy = 0.6614310645724258\n",
      "\n",
      "12/23/2020 16:38:08 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:38:08 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:38:08 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=4 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 420/2001 [00:00<00:00, 4195.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4076.95it/s]\n",
      "Iteration: 100%|██████████| 84/84 [01:33<00:00,  1.12s/it]\n",
      "12/23/2020 16:39:44 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:39:44 - INFO - run_classifier -     test_loss = 1.2607152242036093\n",
      "\n",
      "12/23/2020 16:39:44 - INFO - run_classifier -     5-class test_accuracy = 0.5724258289703316\n",
      "\n",
      "12/23/2020 16:39:44 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:39:44 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:39:44 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=5 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 457/2001 [00:00<00:00, 4565.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4485.11it/s]\n",
      "Iteration: 100%|██████████| 84/84 [01:34<00:00,  1.13s/it]\n",
      "12/23/2020 16:41:21 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:41:21 - INFO - run_classifier -     test_loss = 1.5426092126539774\n",
      "\n",
      "12/23/2020 16:41:21 - INFO - run_classifier -     5-class test_accuracy = 0.4965095986038394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    print(\"===== Word Deletion with K=%s =====\"%(i))\n",
    "    word_deletion_task(\"SST5\", device, sentence_limit=2000, \n",
    "                       k=i, del_type=\"lrp\", \n",
    "                       original_correct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/23/2020 16:30:10 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:30:10 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:30:10 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=1 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 448/2001 [00:00<00:00, 4475.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4393.29it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:31<00:00,  2.67it/s]\n",
      "12/23/2020 16:30:43 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:30:43 - INFO - run_classifier -     test_loss = 0.5505172489654451\n",
      "\n",
      "12/23/2020 16:30:43 - INFO - run_classifier -     5-class test_accuracy = 0.8132635253054101\n",
      "\n",
      "12/23/2020 16:30:43 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:30:43 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:30:43 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=2 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 415/2001 [00:00<00:00, 2409.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 3479.84it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:31<00:00,  2.63it/s]\n",
      "12/23/2020 16:31:17 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:31:17 - INFO - run_classifier -     test_loss = 0.6573992627007621\n",
      "\n",
      "12/23/2020 16:31:17 - INFO - run_classifier -     5-class test_accuracy = 0.7478184991273996\n",
      "\n",
      "12/23/2020 16:31:17 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:31:17 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:31:17 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=3 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 379/2001 [00:00<00:00, 3772.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 3461.02it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:31<00:00,  2.67it/s]\n",
      "12/23/2020 16:31:51 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:31:51 - INFO - run_classifier -     test_loss = 0.8385252998698325\n",
      "\n",
      "12/23/2020 16:31:51 - INFO - run_classifier -     5-class test_accuracy = 0.6867364746945899\n",
      "\n",
      "12/23/2020 16:31:51 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:31:51 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:31:51 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=4 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 445/2001 [00:00<00:00, 4448.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4189.23it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:31<00:00,  2.65it/s]\n",
      "12/23/2020 16:32:24 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:32:24 - INFO - run_classifier -     test_loss = 1.163046630365508\n",
      "\n",
      "12/23/2020 16:32:24 - INFO - run_classifier -     5-class test_accuracy = 0.6134380453752182\n",
      "\n",
      "12/23/2020 16:32:24 - INFO - util.train_helper -   model = BERT\n",
      "12/23/2020 16:32:24 - INFO - util.train_helper -   *** Model Config ***\n",
      "12/23/2020 16:32:24 - INFO - util.train_helper -   {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"full_pooler\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Word Deletion with K=5 =====\n",
      "init_weight = True\n",
      "init_lrp = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 459/2001 [00:00<00:00, 4589.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence limit= 2000\n",
      "0\n",
      "guid= test-0\n",
      "text_a= no movement , no yuks , not much of anything .\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2001/2001 [00:00<00:00, 4232.49it/s]\n",
      "Iteration: 100%|██████████| 84/84 [00:31<00:00,  2.63it/s]\n",
      "12/23/2020 16:32:58 - INFO - run_classifier -   ***** Eval results *****\n",
      "12/23/2020 16:32:58 - INFO - run_classifier -     test_loss = 1.4864931887104398\n",
      "\n",
      "12/23/2020 16:32:58 - INFO - run_classifier -     5-class test_accuracy = 0.5279232111692844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    print(\"===== Word Deletion with K=%s =====\"%(i))\n",
    "    word_deletion_task(\"SST5\", device, sentence_limit=2000, \n",
    "                       k=i, del_type=\"lat\", \n",
    "                       original_correct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp. 3.4 Correlations across datasets\n",
    "Due to the memory limitation and cache limitations, we want run these analysis function 1 at a time to avoid failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst5_dict = analysis_task(\"SST5\", device)\n",
    "torch.save(sst5_dict, \"./sst5_dict.pt\")\n",
    "semeval_dict = analysis_task(\"SemEval\", device)\n",
    "torch.save(semeval_dict, \"./semeval.pt\")\n",
    "imdb_dict = analysis_task(\"IMDb\", device)\n",
    "torch.save(imdb_dict, \"./imdb.pt\")\n",
    "yelp5_dict = analysis_task(\"Yelp5\", device)\n",
    "torch.save(yelp5_dict, \"./yelp5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    sst5_dict = analysis_task(\"SST5\", device)\n",
    "    torch.save(sst5_dict, \"./sst5_dict.pt\")\n",
    "    semeval_dict = analysis_task(\"SemEval\", device)\n",
    "    torch.save(semeval_dict, \"./semeval.pt\")\n",
    "    imdb_dict = analysis_task(\"IMDb\", device)\n",
    "    torch.save(imdb_dict, \"./imdb.pt\")\n",
    "    yelp5_dict = analysis_task(\"Yelp5\", device)\n",
    "    torch.save(yelp5_dict, \"./yelp5.pt\")\n",
    "else:\n",
    "    sst5_dict = torch.load(\"./sst5_dict.pt\")\n",
    "    semeval_dict = torch.load(\"./semeval.pt\")\n",
    "    imdb_dict = torch.load(\"./imdb.pt\")\n",
    "    yelp5_dict = torch.load(\"./yelp5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst5_word_to_score = load_word_score(vocab_data_dir, \n",
    "                                     sst5_dict[\"inputs_ids\"], \n",
    "                                     sst5_dict[\"seqs_lens\"],\n",
    "                                     sst5_dict[\"grad_scores\"])\n",
    "semeval_word_to_score = load_word_score(vocab_data_dir, \n",
    "                                     semeval_dict[\"inputs_ids\"], \n",
    "                                     semeval_dict[\"seqs_lens\"],\n",
    "                                     semeval_dict[\"grad_scores\"])\n",
    "imdb_word_to_score = load_word_score(vocab_data_dir, \n",
    "                                     imdb_dict[\"inputs_ids\"], \n",
    "                                     imdb_dict[\"seqs_lens\"],\n",
    "                                     imdb_dict[\"grad_scores\"])\n",
    "yelp5_word_to_score = load_word_score(vocab_data_dir, \n",
    "                                     yelp5_dict[\"inputs_ids\"], \n",
    "                                     yelp5_dict[\"seqs_lens\"],\n",
    "                                     yelp5_dict[\"grad_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = subset_score([sst5_word_to_score, semeval_word_to_score, imdb_word_to_score, yelp5_word_to_score])\n",
    "score_df = pd.DataFrame({\"sst5\": sst5_word_to_score, \"semeval\": semeval_word_to_score,\n",
    "                         \"imdb\": imdb_word_to_score, \"yelp5\": yelp5_word_to_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['axes.edgecolor'] = \"black\"\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "y = score_list[1]\n",
    "x = score_list[0]\n",
    "\n",
    "plt.scatter(x, y, marker='*', color='r')\n",
    "plt.tight_layout()\n",
    "plt.grid(color='black', linestyle='-.')\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)), color='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def reg_coef(x,y,label=None,color=None,**kwargs):\n",
    "    ax = plt.gca()\n",
    "    r,p = pearsonr(x,y)\n",
    "    ax.annotate('r = {:.2f}'.format(r), xy=(0.5,0.5), xycoords='axes fraction', ha='center', size=30)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "g = sns.PairGrid(score_df)\n",
    "g.map_diag(sns.distplot)\n",
    "g.map_lower(sns.regplot, marker=\"+\", line_kws={\"color\": \"red\"})\n",
    "g.map_upper(reg_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
