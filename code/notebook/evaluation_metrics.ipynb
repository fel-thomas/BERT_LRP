{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-ups and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from model.BiLSTM import *\n",
    "from model.BERT import *\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from util.optimization import BERTAdam\n",
    "from util.processor import *\n",
    "\n",
    "from util.tokenization import *\n",
    "\n",
    "from util.evaluation import *\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# this imports most of the helpers needed to eval the model\n",
    "from run_classifier_sa import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/28/2020 20:25:57 - INFO - run_classifier_sa -   gpu is out of the picture, let us use CPU\n"
     ]
    }
   ],
   "source": [
    "# Note that this notebook only supports single GPU evaluation\n",
    "# which is sufficient for most of tasks by using lower batch size.\n",
    "IS_CUDA = False\n",
    "if IS_CUDA:\n",
    "    CUDA_DEVICE = \"cuda:5\"\n",
    "    device = torch.device(CUDA_DEVICE)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.info(\"device %s in total n_gpu %d distributed training\", device, n_gpu)\n",
    "else:\n",
    "    # bad luck, we are on CPU now!\n",
    "    logger.info(\"gpu is out of the picture, let us use CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME = \"AdvSA\"   # this is the 3-class classification on SST\n",
    "                     # where the classes are \n",
    "                     # \" negative\", \"positive\" and \"neutural\".\n",
    "                     # [0,0.2] && [0.4,0.6] && [0.8,1.0]\n",
    "DATA_DIR = \"../../data/dataset/AdvSA/\"\n",
    "            \n",
    "# \"../../data/uncased_L-12_H-768_A-12/\" is for the default BERT-base pretrain\n",
    "BERT_PATH = \"../../data/uncased_L-12_H-768_A-12/\"\n",
    "MODEL_PATH = \"../../results/\" + TASK_NAME + \"/checkpoint.bin\"\n",
    "EVAL_BATCH_SIZE = 24 # you can tune this down depends on GPU you have.\n",
    "\n",
    "# This loads the task processor for you.\n",
    "processors = {\n",
    "    \"IMDb\":IMDb_Processor,\n",
    "    \"SemEval\":SemEval_Processor,\n",
    "    \"SST5\":SST5_Processor,\n",
    "    \"SST2\":SST2_Processor,\n",
    "    \"SST3\":SST3_Processor,\n",
    "    \"Yelp5\":Yelp5_Processor,\n",
    "    \"Yelp2\":Yelp2_Processor,\n",
    "    \"AdvSA\":AdvSA_Processor\n",
    "}\n",
    "\n",
    "processor = processors[TASK_NAME]()\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specific models, optimizer (not needed), and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/29/2020 01:20:12 - INFO - run_classifier_sa -   model = BERTPretrain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_weight = True\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, tokenizer = \\\n",
    "    getModelOptimizerTokenizer(model_type=\"BERTPretrain\",\n",
    "                               vocab_file=BERT_PATH + \"vocab.txt\",\n",
    "                               embed_file=None,\n",
    "                               bert_config_file=BERT_PATH + \"bert_config.json\",\n",
    "                               init_checkpoint=MODEL_PATH,\n",
    "                               label_list=label_list,\n",
    "                               do_lower_case=True,\n",
    "                               # below is not required for eval\n",
    "                               num_train_steps=20,\n",
    "                               learning_rate=2e-5,\n",
    "                               base_learning_rate=2e-5,\n",
    "                               warmup_proportion=0.1)\n",
    "model = model.to(device) # send the model to device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 219/4500 [00:00<00:01, 2186.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "guid= dev-0\n",
      "text_a= Too expensive?\n",
      "text_b= None\n",
      "label= 0\n",
      "1000\n",
      "guid= dev-1000\n",
      "text_a= I'm not sure where to start.\n",
      "text_b= None\n",
      "label= 2\n",
      "2000\n",
      "guid= dev-2000\n",
      "text_a= I have a 16 year old dog who is having seizures and basically transitioning to the next stage of life.\n",
      "text_b= None\n",
      "label= 0\n",
      "3000\n",
      "guid= dev-3000\n",
      "text_a= Probably your best plan of action.\n",
      "text_b= None\n",
      "label= 1\n",
      "4000\n",
      "guid= dev-4000\n",
      "text_a= Shoot, they should have 3 way everything, 3 way dogs, 3 way salad, 3 way sliders and 3 way pizza.\n",
      "text_b= None\n",
      "label= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4500/4500 [00:02<00:00, 2248.04it/s]\n"
     ]
    }
   ],
   "source": [
    "test_examples = processor.get_dev_examples(DATA_DIR)\n",
    "test_features = \\\n",
    "    convert_examples_to_features(\n",
    "        test_examples,\n",
    "        label_list,\n",
    "        512,\n",
    "        tokenizer)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "all_seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                          all_label_ids, all_seq_len)\n",
    "test_dataloader = DataLoader(test_data, batch_size=EVAL_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Actual evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 188/188 [01:22<00:00,  2.29it/s]\n",
      "10/29/2020 01:22:01 - INFO - run_classifier_sa -   ***** Eval results *****\n",
      "10/29/2020 01:22:01 - INFO - run_classifier_sa -     test_loss = 1.2517471811238756\n",
      "\n",
      "10/29/2020 01:22:01 - INFO - run_classifier_sa -     3-class test_accuracy = 0.5744444444444444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we did not exclude gradients, for attribution methods\n",
    "model.eval() # this line will deactivate dropouts\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "pred_logits = []\n",
    "actual = []\n",
    "# we don't need gradient in this case.\n",
    "for step, batch in enumerate(tqdm(test_dataloader, desc=\"Iteration\")):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    input_ids, input_mask, segment_ids, label_ids, seq_lens = batch\n",
    "    # truncate to save space and computing resource\n",
    "    max_seq_lens = max(seq_lens)[0]\n",
    "    input_ids = input_ids[:,:max_seq_lens]\n",
    "    input_mask = input_mask[:,:max_seq_lens]\n",
    "    segment_ids = segment_ids[:,:max_seq_lens]\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    seq_lens = seq_lens.to(device)\n",
    "\n",
    "    # intentially with gradient\n",
    "    tmp_test_loss, logits = \\\n",
    "        model(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                device=device, labels=label_ids)\n",
    "\n",
    "    logits = F.softmax(logits, dim=-1)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    pred_logits.append(logits)\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    actual.append(label_ids)\n",
    "    outputs = np.argmax(logits, axis=1)\n",
    "    tmp_test_accuracy=np.sum(outputs == label_ids)\n",
    "\n",
    "    test_loss += tmp_test_loss.mean().item()\n",
    "    test_accuracy += tmp_test_accuracy\n",
    "\n",
    "    nb_test_examples += input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "    \n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = collections.OrderedDict()\n",
    "result = {'test_loss': test_loss,\n",
    "            str(len(label_list))+ '-class test_accuracy': test_accuracy}\n",
    "logger.info(\"***** Eval results *****\")\n",
    "for key in result.keys():\n",
    "    logger.info(\"  %s = %s\\n\", key, str(result[key]))\n",
    "# get predictions needed for evaluation\n",
    "pred_logits = np.concatenate(pred_logits, axis=0)\n",
    "actual = np.concatenate(actual, axis=0)\n",
    "pred_label = np.argmax(pred_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.38      0.49      1500\n",
      "           1       0.53      0.63      0.57      1500\n",
      "           2       0.57      0.71      0.63      1500\n",
      "\n",
      "    accuracy                           0.57      4500\n",
      "   macro avg       0.59      0.57      0.57      4500\n",
      "weighted avg       0.59      0.57      0.57      4500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(actual, pred_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it on SST dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_way_classification(pred_logits, actual):\n",
    "    \"\"\"\n",
    "    In a n-class classification problem, we ignore\n",
    "    the rest of the classes, only look at two way\n",
    "    classification accuracy.\n",
    "    \"\"\"\n",
    "    two_way_pred = []\n",
    "    two_way_actual = []\n",
    "    assert len(pred_logits) == len(actual)\n",
    "    for i in range(len(pred_logits)):\n",
    "        if actual\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
